{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "path = './data/'\n",
    "\n",
    "def get_csv(path):\n",
    "    # load CSV & drop zeros\n",
    "    df = pd.read_csv(path+'driving_log.csv')\n",
    "    df = df.drop(df[df['steering'] == 0.].index)\n",
    "    df = df.drop(df[df['speed'] < 1.].index)\n",
    "    return df\n",
    "\n",
    "df = get_csv(path = path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split\n",
    "split_index = int((1-(1/11))*(len(df)))\n",
    "df_train = df[:split_index]\n",
    "df_val  = df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate batch\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "image_shape = cv2.imread( path+df['center'][df['center'].index[0]]).shape\n",
    "batch_size = 7 # 3675 = 3*5*5*7*7\n",
    "\n",
    "def get_batch(df,batch_size): \n",
    "    n_split = int(len(df)//batch_size)-1\n",
    "    batch_idx = (df[:batch_size*n_split].index.values)   \n",
    "    batch_idx = shuffle(batch_idx)\n",
    "    batch_idx = batch_idx.reshape(n_split,batch_size,)\n",
    "    while 1:\n",
    "        for idx in batch_idx:\n",
    "            batch_y_c      =      df['steering'][idx].values\n",
    "            batch_y_r      =      df['steering'][idx].values - 0.02\n",
    "            batch_y_l      =      df['steering'][idx].values + 0.02\n",
    "            batch_y = np.concatenate([batch_y_c,batch_y_r,batch_y_l])\n",
    "            batch_y = np.concatenate([batch_y,-batch_y])\n",
    "\n",
    "            batch_x_path_c = path+df['center'][idx]\n",
    "            batch_x_path_r = path+df['right'][idx]\n",
    "            batch_x_path_l = path+df['left'][idx]\n",
    "            batch_x_path = np.concatenate([batch_x_path_c,batch_x_path_r,batch_x_path_l])\n",
    "\n",
    "            batch_x      = np.empty(list([batch_size*3]) + list(image_shape))\n",
    "            ii = 0\n",
    "            for img_path in batch_x_path:\n",
    "                image = cv2.imread(img_path)\n",
    "                #image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS) ## COLOR_RGB2HSV COLOR_RGB2HLS\n",
    "                batch_x[ii] = image\n",
    "                ii +=1\n",
    "                \n",
    "\n",
    "            batch_x = np.concatenate([batch_x,batch_x[:,:,-1::-1]])\n",
    "\n",
    "\n",
    "            assert(not(np.isnan((np.sum(batch_x)))))\n",
    "            assert(not(np.isnan((np.sum(batch_y)))))\n",
    "            yield shuffle(batch_x,batch_y) \n",
    "\n",
    "train_generator = get_batch(df_train, batch_size=batch_size)\n",
    "validation_generator = get_batch(df_val, batch_size=batch_size)\n",
    "\n",
    "batch_x,batch_y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_dataset(image,label,steps):\n",
    "    plt_num = 1\n",
    "    print(image.shape)\n",
    "    step_size = int(image.shape[0]/steps)\n",
    "    for image_idx in range(0,image.shape[0],step_size):\n",
    "        channels = image.shape[3]\n",
    "        plt.figure(plt_num, figsize=(32,32))\n",
    "        for channel in range(channels):\n",
    "            plt.subplot(4,8, channel+1) # sets the number of feature maps to show on each row and column\n",
    "            plt.title('channel ' + str(channel)) # displays the feature map number\n",
    "            plt.imshow(image[image_idx,:,:, channel], interpolation=\"nearest\", cmap=\"gray\")\n",
    "        plt.show()\n",
    "        print('{1}, idx: {0}'.format(image_idx,label[image_idx],))\n",
    "        \n",
    "#%time visualise_dataset(batch_x,batch_y,steps=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout, Reshape, Flatten, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Merge\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "\n",
    "# Hyperparameter Compile\n",
    "loss= 'mse' # 'hinge'\n",
    "optimizer= 'Nadam' #'Nadam' #'rmsprop'\n",
    "\n",
    "# Hyperparameter Fit\n",
    "nb_epoch= 10 * 5\n",
    "\n",
    "try:\n",
    "    model = load_model('model.h5')\n",
    "    print(\"load_model done\")\n",
    "except:\n",
    "    print(\"load_model failed\")\n",
    "    \n",
    "def model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Cropping2D(cropping=((50,25), (0,0)), input_shape=image_shape))\n",
    "    model.add(Lambda(lambda x: (x[:,:,:,0:1]+x[:,:,:,1:2]+x[:,:,:,2:3])/3)) ## drive on red\n",
    "    model.add(Lambda(lambda x: x/127.5 - 1.))\n",
    "    return model\n",
    "\n",
    "def model_1hsv():\n",
    "    model = Sequential()\n",
    "    model.add(Cropping2D(cropping=((50,25), (0,0)), input_shape=image_shape))\n",
    "    model.add(Lambda(lambda x: matplotlib.colors.rgb_to_hsv(x)))\n",
    "    model.add(Lambda(lambda x: (x[:,:,:,1:3]))) \n",
    "    return model\n",
    "\n",
    "def model_1mirror():\n",
    "    model1 = model_1()\n",
    "    model2 = model1\n",
    "    model2.add(Lambda(lambda x: x[:,:,-1::-1]))\n",
    "    model2.add(Lambda(lambda x: matplotlib.colors.rgb_to_hsv(x)))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Merge([model2, model1]))\n",
    "    return model\n",
    "\n",
    "def model_3():\n",
    "    model = model_1()\n",
    "    model.add(Convolution2D(6,6,1,activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(6,6,1,activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(6,6,1,activation='relu'))    \n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(300))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dropout(0.90))\n",
    "    model.add(Dense(120))\n",
    "    model.add(Dense(84))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    return model\n",
    "\n",
    "model = model_3()\n",
    "\n",
    "\n",
    "%time model.compile(loss=loss, optimizer=optimizer)\n",
    "%time history_object = model.fit_generator(generator         = train_generator, \\\n",
    "                    samples_per_epoch = batch_size*6, \\\n",
    "                    validation_data   = validation_generator, \\\n",
    "                    nb_val_samples    = 1, \\\n",
    "                    nb_epoch          = nb_epoch)\n",
    "\n",
    "model.save(\"model.h5\")\n",
    "print('model saved')\n",
    "#model = load_model('model.h5')\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#model.save(\"model.h5\")\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_StratifiedShuffleSplit(batches_idx,batch_size):\n",
    "    n_bins  = 5\n",
    "    batch_size = 5*7*7\n",
    "    y_classes = df['steering'].copy()*n_bins//1\n",
    "    batches_idx = df.index\n",
    "    data_size = len(batches_idx)\n",
    "    n_split = int(data_size//batch_size)-1\n",
    "    print(n_split)\n",
    "    batch_range = df.index\n",
    "    sss = StratifiedShuffleSplit(n_splits = n_split,test_size = 15)\n",
    "    batch_idx, val_idx = next(sss.split((batch_range),(y_classes.values)))\n",
    "    \n",
    "    batch_idx = batch_idx[:batch_size*n_split].reshape((n_split,batch_size, ))\n",
    "    batch_idx = batch_idx[:batch_size*n_split].reshape((n_split,batch_size, ))\n",
    "    \n",
    "    return batch_idx, val_idx\n",
    "\n",
    "    batch_idx, val_idx = (get_batch_idx(batches_idx,batch_size = 256))\n",
    "    batch_idx.shape\n",
    "    \n",
    "    #from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "#from keras.layers.wrappers import TimeDistributed\n",
    "#from keras.regularizers import l2, activity_l2\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
